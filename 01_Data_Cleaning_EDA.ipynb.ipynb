{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Globalcom telecom Customer Churn Project: Data Preparation & EDA\n",
        "\n",
        "**Author:** Akinradewo Aarinola Olamiposi  \n",
        "**Date:** 21 September, 2025    \n",
        "**Description:** This notebook outlines the data cleaning, transformation, and initial exploratory data analysis (EDA) for the hypothetical Telco customer churn project.\n"
      ],
      "metadata": {
        "id": "cx9QANi9-dcI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qK0_ThHt98_2"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Data Manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# Print all outputs from a cell, not just the last one\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load each individual dataset and get a first look to understand its structure."
      ],
      "metadata": {
        "id": "QJR1bRS2Chsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Load and Inspect Raw Datasets\n",
        "\n",
        "# 1. Load the data from CSV files\n",
        "df_demographics = pd.read_csv('../data/raw/customer_demographics.csv')\n",
        "df_services = pd.read_csv('../data/raw/customer_services.csv')\n",
        "df_support = pd.read_csv('../data/raw/support_tickets.csv')\n",
        "\n",
        "# 2. Inspect each DataFrame one by one\n",
        "print(\"DEMOGRAPHICS DATA:\")\n",
        "print(\"Shape:\", df_demographics.shape)\n",
        "df_demographics.head(2)\n",
        "print(\"\\n\") # Adds a space for readability\n",
        "\n",
        "print(\"SERVICES DATA:\")\n",
        "print(\"Shape:\", df_services.shape)\n",
        "df_services.head(2)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"SUPPORT TICKETS DATA:\")\n",
        "print(\"Shape:\", df_support.shape)\n",
        "df_support.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "s7t76GH__D1k",
        "outputId": "2f846f41-4491-46e3-cc3c-cd997975e1e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../data/raw/customer_demographics.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3825567151.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 1. Load the data from CSV files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_demographics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/raw/customer_demographics.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf_services\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/raw/customer_services.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_support\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/raw/support_tickets.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/raw/customer_demographics.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To combine the separate DataFrames from Cell 2 into one master dataset."
      ],
      "metadata": {
        "id": "wuQSkTgVCd8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Merge and Integrate Data\n",
        "\n",
        "# 1. Merge demographics and services data on customerID\n",
        "df_merged = pd.merge(df_demographics, df_services, on='customerID', how='inner')\n",
        "print(\"Shape after merging demographics and services:\", df_merged.shape)\n",
        "\n",
        "# 2. Aggregate support tickets: count tickets per customer\n",
        "# We need to transform the support log into a customer-level summary\n",
        "df_support_agg = df_support.groupby('customerID').size().reset_index(name='number_of_support_tickets')\n",
        "print(\"Unique customers in support logs:\", df_support_agg.shape[0])\n",
        "\n",
        "# 3. Merge the support ticket count into the main dataframe\n",
        "df_main = pd.merge(df_merged, df_support_agg, on='customerID', how='left')\n",
        "\n",
        "# 4. Fill NaN values for customers with no support tickets\n",
        "df_main['number_of_support_tickets'].fillna(0, inplace=True)\n",
        "\n",
        "# 5. Inspect the final merged dataset\n",
        "print(\"\\nFINAL MERGED DATASET:\")\n",
        "print(\"Shape:\", df_main.shape)\n",
        "print(\"\\nFirst 3 rows:\")\n",
        "df_main.head(3)\n",
        "\n",
        "# Show info to check for missing values after merge\n",
        "print(\"\\nInfo:\")\n",
        "df_main.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "HzOynGbL_eN5",
        "outputId": "8f420e9e-3bc9-4585-9933-f6fcc61a0b67"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_demographics' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1589702171.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 1. Merge demographics and services data on customerID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_merged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_demographics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_services\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'customerID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape after merging demographics and services:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_demographics' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check for data quality issues in the newly merged DataFrame (df_main)."
      ],
      "metadata": {
        "id": "oDVjwFyrBgss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Initial Data Assessment\n",
        "print(\"MISSING VALUES:\")\n",
        "print(df_main.isnull().sum())\n",
        "print(\"\\nDATA TYPES:\")\n",
        "print(df_main.dtypes)\n",
        "print(f\"\\nDUPLICATE CUSTOMER IDs: {df_main.duplicated(subset=['customerID']).sum()}\")\n",
        "df_main.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "kBDF5YsEAgYg",
        "outputId": "b56b5158-da2b-4c12-f411-91f06cfc8797"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MISSING VALUES:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_main' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-278518695.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Cell 4: Initial Data Assessment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MISSING VALUES:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_main\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDATA TYPES:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_main\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_main' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0tE4hyu9Bbrb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fix the issues found in Cell 4."
      ],
      "metadata": {
        "id": "geg5SRd9Bzv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Data Cleaning\n",
        "# Convert TotalCharges to numeric, forcing errors to NaN\n",
        "df_main['TotalCharges'] = pd.to_numeric(df_main['TotalCharges'], errors='coerce')\n",
        "# Check for the new missing values created by conversion\n",
        "print(\"Customers with missing TotalCharges after conversion:\")\n",
        "print(df_main[df_main['TotalCharges'].isnull()][['customerID', 'tenure', 'MonthlyCharges', 'TotalCharges']])\n",
        "# Impute missing TotalCharges with 0 for customers with tenure=0\n",
        "df_main['TotalCharges'].fillna(0, inplace=True)\n",
        "# Standardize categorical values\n",
        "df_main['PaymentMethod'].replace({'e-check': 'Electronic check'}, inplace=True)\n",
        "print(\"Missing values after cleaning:\", df_main.isnull().sum().sum())"
      ],
      "metadata": {
        "id": "tTzKU7htA4Lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create new, more powerful features from the existing clean data."
      ],
      "metadata": {
        "id": "t9tJpFbGB4A2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Feature Engineering\n",
        "# Create TenureGroup feature\n",
        "def tenure_group(tenure):\n",
        "    if tenure < 12:\n",
        "        return '0-12'\n",
        "    elif tenure < 24:\n",
        "        return '13-24'\n",
        "    elif tenure < 36:\n",
        "        return '25-36'\n",
        "    elif tenure < 48:\n",
        "        return '37-48'\n",
        "    else:\n",
        "        return '49+'\n",
        "df_main['TenureGroup'] = df_main['tenure'].apply(tenure_group)\n",
        "# Display new features\n",
        "df_main[['customerID', 'tenure', 'TenureGroup']].head()"
      ],
      "metadata": {
        "id": "qksIlJWKA5od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize the data and start generating insights"
      ],
      "metadata": {
        "id": "1Lf-_psiCA4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Exploratory Data Analysis (EDA)\n",
        "# Set visual style\n",
        "sns.set_style(\"whitegrid\")\n",
        "# 1. Churn Count Plot\n",
        "plt.figure(figsize=(6,4))\n",
        "df_main['Churn'].value_counts().plot(kind='bar', color=['skyblue', 'salmon'])\n",
        "plt.title('Overall Customer Churn Count')\n",
        "plt.ylabel('Number of Customers')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "44RGB-4fA_ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To export the cleaned and processed data for the next stage."
      ],
      "metadata": {
        "id": "AvTTve9YCHWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Save the Final Dataset\n",
        "df_main.to_csv('../data/processed/telco_churn_clean.csv', index=False)\n",
        "print(\"Clean dataset saved successfully. Ready for modeling!\")\n",
        "print(f\"Final Dataset Shape: {df_main.shape}\")"
      ],
      "metadata": {
        "id": "a5CLEmSnBEoj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}